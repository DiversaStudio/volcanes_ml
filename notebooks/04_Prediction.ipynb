{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set path\nproject_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\nif project_root not in sys.path:\n    sys.path.append(project_root)\n\n# Load configuration\nfrom src.config import get_config\nconfig = get_config()\n\nprint(\"✓ Configuration loaded successfully\")\nprint(f\"  Model classes: {config.model['n_classes']}\")\nprint(f\"  Test data path: {config.paths['test']}\")\nprint(f\"  Results path: {config.paths['results']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from src.data.loader import ThermalDataLoader\n",
    "from src.features.thermal import create_thermal_threshold_tensor\n",
    "from src.features.edge_detection import create_edge_detection_tensors\n",
    "from src.models.multibranch import MultiBranchModel\n",
    "from src.data.dataset import ThermalDataset\n",
    "from src.utils.visualization import (\n",
    "    get_thermal_stats,\n",
    "    print_thermal_stats,\n",
    "    visualize_thermal_sequence,\n",
    "    get_edge_stats,\n",
    "    print_edge_stats,\n",
    "    visualize_edge_sequence,\n",
    "    visualize_thermal_threshold_comparison,\n",
    "    get_label_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_path, is_single_file=False):\n",
    "    \"\"\"\n",
    "    Preprocess new thermal data for prediction following the original pipeline.\n",
    "    \"\"\"\n",
    "    # Initialize data loader\n",
    "    if is_single_file:\n",
    "        base_directories = [os.path.dirname(data_path)]\n",
    "    else:\n",
    "        base_directories = [data_path]\n",
    "        \n",
    "    thermal_loader = ThermalDataLoader(\n",
    "        base_directories=base_directories,\n",
    "        allowed_labels=None  # No labels needed for prediction\n",
    "    )\n",
    "    \n",
    "    # Create initial dataset\n",
    "    print(\"\\nCreating dataset...\")\n",
    "    dataset = thermal_loader.create_dataset()\n",
    "    \n",
    "    if dataset is not None:\n",
    "        # Follow the same preprocessing steps as in DataPreprocessing.py\n",
    "        print(\"\\nPreprocessing data...\")\n",
    "        \n",
    "        # Convert to tensor and handle NaN values\n",
    "        corrected_tensor = torch.tensor(dataset['tensors']['corrected']).float()\n",
    "        if torch.isnan(corrected_tensor).any():\n",
    "            print(\"Warning: NaN values found in corrected tensor!\")\n",
    "            corrected_tensor = torch.nan_to_num(corrected_tensor, nan=0.0)\n",
    "        \n",
    "        # Create edge detection tensors\n",
    "        print(\"Creating edge detection tensors...\")\n",
    "        edge_detection_data = create_edge_detection_tensors(corrected_tensor)\n",
    "        edge_tensor = torch.tensor(edge_detection_data['edge_tensor']).float()\n",
    "        edge_tensor = torch.nan_to_num(edge_tensor, nan=0.0)\n",
    "        \n",
    "        # Create threshold tensors\n",
    "        print(\"Creating threshold tensors...\")\n",
    "        threshold_data = create_thermal_threshold_tensor(corrected_tensor)\n",
    "        for name, tensor in threshold_data['tensors'].items():\n",
    "            threshold_data['tensors'][name] = torch.nan_to_num(tensor, nan=0.0)\n",
    "        \n",
    "        # Create preprocessed dataset dictionary (following original structure)\n",
    "        preprocessed_dataset = {\n",
    "            'tensors': {\n",
    "                'corrected': corrected_tensor,\n",
    "                'edge': edge_tensor,\n",
    "                'threshold': threshold_data['tensors']\n",
    "            },\n",
    "            'metadata': dataset['metadata']\n",
    "        }\n",
    "        \n",
    "        # Apply the same permutations as in FeatureEngineering.py\n",
    "        print(\"\\nReorganizing tensors...\")\n",
    "        preprocessed_dataset['tensors']['corrected'] = preprocessed_dataset['tensors']['corrected'].permute(2, 0, 1)\n",
    "        preprocessed_dataset['tensors']['edge'] = preprocessed_dataset['tensors']['edge'].permute(2, 0, 1)\n",
    "        \n",
    "        # Handle threshold tensors permutation\n",
    "        threshold_tensors = []\n",
    "        for level in preprocessed_dataset['tensors']['threshold'].keys():\n",
    "            permuted_tensor = preprocessed_dataset['tensors']['threshold'][level].permute(2, 0, 1)\n",
    "            threshold_tensors.append(permuted_tensor)\n",
    "        \n",
    "        # Stack threshold tensors\n",
    "        stacked_thresholds = torch.stack(threshold_tensors, dim=1)\n",
    "        \n",
    "        # Add channel dimension for corrected and edge tensors\n",
    "        corrected = preprocessed_dataset['tensors']['corrected'].unsqueeze(1)\n",
    "        edge = preprocessed_dataset['tensors']['edge'].unsqueeze(1)\n",
    "        \n",
    "        print(\"\\nFinal tensor shapes:\")\n",
    "        print(f\"Corrected: {corrected.shape}\")\n",
    "        print(f\"Edge: {edge.shape}\")\n",
    "        print(f\"Thresholds: {stacked_thresholds.shape}\")\n",
    "        \n",
    "        return {\n",
    "            'corrected': corrected,\n",
    "            'edge': edge,\n",
    "            'thresholds': stacked_thresholds,\n",
    "            'metadata': dataset['metadata']\n",
    "        }\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_model_and_predict(model_path, preprocessed_data):\n    \"\"\"\n    Load the trained model and make predictions.\n    \"\"\"\n    print(\"\\nLoading model and making predictions...\")\n    \n    # Load model with configuration\n    n_classes = config.model['n_classes']\n    model = MultiBranchModel(n_classes).to(device)\n    \n    print(f\"Model initialized with {n_classes} classes\")\n    \n    # Load saved weights\n    checkpoint = torch.load(model_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    \n    # Prepare data\n    with torch.no_grad():\n        corrected = preprocessed_data['corrected'].float().to(device)\n        edge = preprocessed_data['edge'].float().to(device)\n        thresholds = preprocessed_data['thresholds'].float().to(device)\n        \n        # Get predictions\n        outputs = model(corrected, edge, thresholds)\n        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n        _, predicted = torch.max(outputs, 1)\n    \n    return predicted.cpu().numpy(), probabilities.cpu().numpy()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def export_predictions_to_csv(predictions, probabilities, metadata, input_dir, save_dir):\n    \"\"\"\n    Export predictions to CSV files.\n    \n    Args:\n        predictions: numpy array of predicted classes\n        probabilities: numpy array of prediction probabilities\n        metadata: dictionary containing timestamps and file information\n        save_dir: directory to save CSV files\n    \"\"\"\n        \n    # Create results directory if it doesn't exist\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Current timestamp for file names\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n    \n    # Get file paths and timestamps from metadata\n    file_timestamps = metadata['timestamps']  # This should be a list of timestamps from the files\n    # Get list of files from input directory\n    filenames = sorted([f for f in os.listdir(input_dir) if f.endswith('.fff')])\n    \n    # 1. Detailed predictions\n    detailed_results = []\n    for fname, pred, probs in zip(filenames, predictions, probabilities):\n        result_row = {\n            'file_name': fname,\n            'prediction': label_mapping[pred],\n            'confidence': probs[pred] * 100,  # Convert to percentage\n        }\n        # Add probability columns for each class (all 4 classes)\n        for idx, label in label_mapping.items():\n            result_row[f'prob_{label.lower()}'] = probs[idx] * 100\n        detailed_results.append(result_row)\n    \n    # Save detailed results\n    detailed_df = pd.DataFrame(detailed_results)\n    detailed_path = os.path.join(save_dir, f'detailed_predictions_{timestamp}.csv')\n    detailed_df.to_csv(detailed_path, index=False)\n    \n    # 2. Summary statistics\n    unique_predictions, counts = np.unique(predictions, return_counts=True)\n    summary_results = []\n    for pred, count in zip(unique_predictions, counts):\n        summary_results.append({\n            'class': label_mapping[pred],\n            'count': int(count),\n            'percentage': float(count/len(predictions)*100)\n        })\n    \n    # Save summary\n    summary_df = pd.DataFrame(summary_results)\n    summary_path = os.path.join(save_dir, f'summary_predictions_{timestamp}.csv')\n    summary_df.to_csv(summary_path, index=False)\n    \n    print(f\"\\n✓ Detailed predictions saved to: {detailed_path}\")\n    print(f\"✓ Summary saved to: {summary_path}\")\n    \n    # Also print summary to console\n    print(\"\\nPrediction Summary:\")\n    print(summary_df.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define label mapping from configuration\nlabel_mapping = {\n    config.labels['Despejado']: 'Despejado',     # Clear/No activity\n    config.labels['Nublado']: 'Nublado',         # Cloudy conditions\n    config.labels['Emisiones']: 'Emisiones',     # Emissions\n    config.labels['Flujo']: 'Flujo'              # Lava flow\n}\n\nprint(\"Label mapping:\")\nfor idx, label in label_mapping.items():\n    print(f\"  {idx}: {label}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if __name__ == \"__main__\":\n    # Set paths using configuration\n    model_path = os.path.join(project_root, config.paths['models'], 'best_model.pt')\n    data_path = os.path.join(project_root, config.paths['test'], 'DiarioRaw')\n    save_dir = os.path.join(project_root, config.paths['results'])\n    \n    print(f\"Model path: {model_path}\")\n    print(f\"Data path: {data_path}\")\n    print(f\"Results directory: {save_dir}\")\n    \n    # Process data\n    print(\"\\nProcessing data...\")\n    preprocessed_data = preprocess_data(data_path, is_single_file=False)\n    \n    if preprocessed_data is not None:\n        # Make predictions (only once)\n        predictions, probabilities = load_model_and_predict(model_path, preprocessed_data)\n        \n        # Save torch results\n        results = {\n            'predictions': predictions,\n            'probabilities': probabilities\n        }\n        results_path = os.path.join(save_dir, 'predictions.pt')\n        os.makedirs(os.path.dirname(results_path), exist_ok=True)\n        torch.save(results, results_path)\n        print(f\"\\n✓ Torch results saved to: {results_path}\")\n        \n        # Export to CSV\n        export_predictions_to_csv(\n            predictions, \n            probabilities, \n            preprocessed_data['metadata'],\n            data_path,  # input directory\n            save_dir\n        )\n    else:\n        print(\"Error: Failed to preprocess data\")"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(preprocessed_data, predictions, probabilities, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualize thermal images with their predictions.\n",
    "    \n",
    "    Args:\n",
    "        preprocessed_data: Dictionary containing the preprocessed tensors and metadata\n",
    "        predictions: numpy array of predicted classes\n",
    "        probabilities: numpy array of prediction probabilities\n",
    "        num_samples: number of samples to visualize per class (default=3)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    \n",
    "    # Get the corrected tensor and metadata\n",
    "    corrected_tensor = preprocessed_data['corrected'].squeeze(1)  # Remove channel dimension\n",
    "    timestamps = preprocessed_data['metadata']['timestamps']\n",
    "    \n",
    "    # Calculate thermal stats\n",
    "    thermal_stats = {\n",
    "        'global_min': torch.min(corrected_tensor).item(),\n",
    "        'global_max': torch.max(corrected_tensor).item(),\n",
    "        'mean_temp': torch.mean(corrected_tensor).item(),\n",
    "        'std_temp': torch.std(corrected_tensor).item()\n",
    "    }\n",
    "    \n",
    "    # Print thermal statistics\n",
    "    print(\"\\nTemperature Statistics:\")\n",
    "    print(f\"Global min: {thermal_stats['global_min']:.2f}°C\")\n",
    "    print(f\"Global max: {thermal_stats['global_max']:.2f}°C\")\n",
    "    print(f\"Mean temperature: {thermal_stats['mean_temp']:.2f}°C\")\n",
    "    print(f\"Standard deviation: {thermal_stats['std_temp']:.2f}°C\")\n",
    "    \n",
    "    # Get sample indices for each class\n",
    "    class_indices = {}\n",
    "    for class_idx in range(len(label_mapping)):\n",
    "        class_samples = np.where(predictions == class_idx)[0]\n",
    "        if len(class_samples) > 0:\n",
    "            # Take up to num_samples samples for each class\n",
    "            class_indices[class_idx] = class_samples[:num_samples]\n",
    "    \n",
    "    # Create visualization\n",
    "    total_classes = len(class_indices)\n",
    "    \n",
    "    # Create figure and axes\n",
    "    fig, axes = plt.subplots(total_classes, num_samples, \n",
    "                            figsize=(5 * num_samples, 5 * total_classes))\n",
    "    \n",
    "    # Make axes 2D even when there's only one row or column\n",
    "    if total_classes == 1:\n",
    "        axes = np.array([axes])\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(total_classes, 1)\n",
    "    \n",
    "    for row, (class_idx, indices) in enumerate(class_indices.items()):\n",
    "        for col, idx in enumerate(indices):\n",
    "            if col < num_samples:\n",
    "                ax = axes[row, col]\n",
    "                \n",
    "                # Plot thermal image\n",
    "                im = ax.imshow(\n",
    "                    corrected_tensor[idx].cpu(),\n",
    "                    cmap='inferno',\n",
    "                    vmin=thermal_stats['global_min'],\n",
    "                    vmax=thermal_stats['global_max']\n",
    "                )\n",
    "                \n",
    "                # Add title with timestamp and predictions\n",
    "                title = f\"Time: {timestamps[idx]}\\n\"\n",
    "                title += f\"Prediction: {label_mapping[class_idx]}\\n\"\n",
    "                title += f\"Confidence: {probabilities[idx][class_idx]*100:.1f}%\"\n",
    "                ax.set_title(title)\n",
    "                \n",
    "                # Remove ticks\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                \n",
    "                # Add colorbar\n",
    "                divider = make_axes_locatable(ax)\n",
    "                cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "                plt.colorbar(im, cax=cax)\n",
    "            \n",
    "            # If we have fewer samples than num_samples, hide empty subplots\n",
    "            if col >= len(indices):\n",
    "                axes[row, col].axis('off')\n",
    "    \n",
    "    plt.suptitle('Thermal Images with Predictions', y=1.02, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if __name__ == \"__main__\":\n    # Set paths using configuration\n    save_dir = os.path.join(project_root, config.paths['results'])\n    results_path = os.path.join(save_dir, 'predictions.pt')\n    data_path = os.path.join(project_root, config.paths['test'], 'DiarioRaw')\n    \n    # Check if preprocessed data and results exist\n    if os.path.exists(results_path):\n        # Load saved results\n        results = torch.load(results_path)\n        predictions = results['predictions']\n        probabilities = results['probabilities']\n        \n        # Load and preprocess data again (needed for visualization)\n        preprocessed_data = preprocess_data(data_path, is_single_file=False)\n        \n        if preprocessed_data is not None:\n            # Generate visualization with 3 samples per class\n            print(\"\\nGenerating visualization...\")\n            fig = visualize_predictions(preprocessed_data, predictions, probabilities, num_samples=3)\n            \n            # Save the figure\n            viz_path = os.path.join(save_dir, 'thermal_predictions.png')\n            fig.savefig(viz_path, bbox_inches='tight', dpi=300)\n            print(f\"✓ Visualization saved to: {viz_path}\")\n        else:\n            print(\"Error: Failed to load data for visualization\")\n    else:\n        print(f\"Error: No prediction results found at {results_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volcanesML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}