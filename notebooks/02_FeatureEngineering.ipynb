{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.visualization import (\n",
    "    get_thermal_stats,\n",
    "    print_thermal_stats,\n",
    "    visualize_thermal_sequence,\n",
    "    get_edge_stats,\n",
    "    print_edge_stats,\n",
    "    visualize_edge_sequence,\n",
    "    visualize_thermal_threshold_comparison,\n",
    "    get_label_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading preprocessed dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco\\AppData\\Local\\Temp\\ipykernel_22756\\4270096148.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  preprocessed_dataset = torch.load(preprocessed_dataset_path)\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed dataset\n",
    "print(\"\\nLoading preprocessed dataset...\")\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "preprocessed_dataset_path = os.path.join(project_root, 'data', 'processed', 'preprocessed_dataset.pt')\n",
    "preprocessed_dataset = torch.load(preprocessed_dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatting tensors...\n"
     ]
    }
   ],
   "source": [
    "# Convert tensors to correct format for model\n",
    "print(\"\\nFormatting tensors...\")\n",
    "corrected_tensor = preprocessed_dataset['tensors']['corrected'].permute(2, 0, 1).unsqueeze(1)  # (N, 1, H, W)\n",
    "edge_tensor = preprocessed_dataset['tensors']['edge'].permute(2, 0, 1).unsqueeze(1)            # (N, 1, H, W)\n",
    "threshold_tensor = preprocessed_dataset['tensors']['threshold']['low'].permute(2, 0, 1).unsqueeze(1)  # (N, 1, H, W)\n",
    "temporal_tensor = preprocessed_dataset['tensors']['temporal'].permute(2, 3, 0, 1)  # (N, 6, H, W)\n",
    "labels = preprocessed_dataset['labels']['numeric_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor shapes after formatting:\n",
      "Corrected tensor: torch.Size([15, 1, 480, 640])\n",
      "Edge tensor: torch.Size([15, 1, 480, 640])\n",
      "Threshold tensor: torch.Size([15, 1, 480, 640])\n",
      "Temporal tensor: torch.Size([15, 6, 480, 640])\n",
      "Labels: torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "# Print tensor shapes\n",
    "print(\"\\nTensor shapes after formatting:\")\n",
    "print(f\"Corrected tensor: {corrected_tensor.shape}\")\n",
    "print(f\"Edge tensor: {edge_tensor.shape}\")\n",
    "print(f\"Threshold tensor: {threshold_tensor.shape}\")\n",
    "print(f\"Temporal tensor: {temporal_tensor.shape}\")\n",
    "print(f\"Labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create refined Dataset class for model\n",
    "class ThermalDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data, indices=None, transform=None):\n",
    "        self.data = preprocessed_data\n",
    "        self.indices = indices if indices is not None else range(len(preprocessed_data['tensors']['corrected']))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        sample = {\n",
    "            'corrected': self.data['tensors']['corrected'][real_idx],\n",
    "            'edge': self.data['tensors']['edge'][real_idx],\n",
    "            'temporal': self.data['tensors']['temporal'][real_idx],\n",
    "            'threshold': self.data['tensors']['threshold']['low'][real_idx],\n",
    "            'label': self.data['labels']['numeric_labels'][real_idx]\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data into train and validation sets...\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and validation sets\n",
    "print(\"\\nSplitting data into train and validation sets...\")\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(corrected_tensor)), \n",
    "    test_size=0.2, \n",
    "    stratify=labels,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ThermalDataset(preprocessed_dataset, indices=train_idx)\n",
    "val_dataset = ThermalDataset(preprocessed_dataset, indices=val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits:\n",
      "Training samples: 12\n",
      "Validation samples: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset splits:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying data loading...\n",
      "\n",
      "Batch shapes:\n",
      "Corrected: torch.Size([12, 640, 15])\n",
      "Edge: torch.Size([12, 640, 15])\n",
      "Temporal: torch.Size([12, 640, 15, 6])\n",
      "Threshold: torch.Size([12, 640, 15])\n",
      "Labels: torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "# Verify data loading\n",
    "print(\"\\nVerifying data loading...\")\n",
    "for batch in train_loader:\n",
    "    print(\"\\nBatch shapes:\")\n",
    "    print(f\"Corrected: {batch['corrected'].shape}\")\n",
    "    print(f\"Edge: {batch['edge'].shape}\")\n",
    "    print(f\"Temporal: {batch['temporal'].shape}\")\n",
    "    print(f\"Threshold: {batch['threshold'].shape}\")\n",
    "    print(f\"Labels: {batch['label'].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training class distribution:\n",
      "Class 0: 4 samples (33.33%)\n",
      "Class 1: 4 samples (33.33%)\n",
      "Class 2: 4 samples (33.33%)\n",
      "\n",
      "Validation class distribution:\n",
      "Class 0: 1 samples (33.33%)\n",
      "Class 1: 1 samples (33.33%)\n",
      "Class 2: 1 samples (33.33%)\n"
     ]
    }
   ],
   "source": [
    "# Print class distribution in splits\n",
    "def print_class_distribution(dataset, split_name):\n",
    "    labels = [data['label'].item() for data in dataset]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"\\n{split_name} class distribution:\")\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        print(f\"Class {label}: {count} samples ({count/len(dataset)*100:.2f}%)\")\n",
    "\n",
    "print_class_distribution(train_dataset, \"Training\")\n",
    "print_class_distribution(val_dataset, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volcanesML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
